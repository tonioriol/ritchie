# AGENTS.md

This file provides guidance to agents when working with code in this repository.

## Stack & repo layout (non-obvious)

- Tooling is provided via Devbox + direnv: [`.envrc`](.envrc:1) loads `devbox` and `dotenv_if_exists`, then exports `KUBECONFIG=${PWD}/clusters/neumann/kubeconfig`.
- Secrets live in [`.env`](.env:1) (gitignored by [`.gitignore`](.gitignore:1)); generated kubeconfigs are also ignored (`clusters/*/kubeconfig` and top-level `kubeconfig`).
- GitOps “app-of-apps”: [`apps/root.yaml`](apps/root.yaml:1) points ArgoCD at this repo (`path: apps`) and auto-syncs (`prune` + `selfHeal`). Each file in `apps/` is an ArgoCD `Application`.
- Helm charts live in `charts/` and are referenced by ArgoCD `Application.spec.source.path` (e.g. [`apps/acestream.yaml`](apps/acestream.yaml:1) -> `charts/acestream`).

## Commands / validation (there is no unit-test suite)

- Enter the toolchain (installs `kubectl`, `helm`, `argocd`): `devbox shell` (or `direnv allow` if using direnv).
- Validate Helm charts locally:
  - `helm lint charts/acestream`
  - `helm lint charts/argocd-ingress`
  - (render check) `helm template test charts/acestream | kubectl apply --dry-run=server -f -`
- Apply GitOps manifests (when needed): `kubectl apply -f apps/root.yaml` (ArgoCD will reconcile the rest).

## Acestreamio release process (addon repo)

- The addon repo (`tonioriol/acestreamio`) uses semantic-release on every push to `main`.
- Use Conventional Commits (e.g. `fix:`/`feat:`) so semantic-release can compute SemVer.
- The `Release` workflow creates the tag/release and builds `ghcr.io/tonioriol/acestreamio:vX.Y.Z`.
- ArgoCD Image Updater in this repo detects the new SemVer tag and rolls the deployment automatically.

## Cluster/provisioning gotchas

- `hetzner-k3s` uses `HCLOUD_TOKEN` (not `HETZNER_TOKEN`) per the event log in [`docs/feat/2026-01-19-21-13-09-feat-k3-cluster-hetzner/context.md`](docs/feat/2026-01-19-21-13-09-feat-k3-cluster-hetzner/context.md:333).
- Single-node cluster scheduling relies on `schedule_workloads_on_masters: true` in [`clusters/neumann/cluster.yaml`](clusters/neumann/cluster.yaml:24).

## ArgoCD ingress/TLS redirect-loop fix

- TLS is terminated at Traefik; ArgoCD server must run “insecure” to avoid redirect loops: [`manifests/argocd/argocd-cmd-params-cm.yaml`](manifests/argocd/argocd-cmd-params-cm.yaml:1) sets `server.insecure: "true"`.
- That config is kept in sync via [`apps/argocd-config.yaml`](apps/argocd-config.yaml:1).

## Metrics-server (k3s TLS)

- `kubectl top …` works because [`apps/metrics-server.yaml`](apps/metrics-server.yaml:1) injects `--kubelet-insecure-tls` (k3s kubelet certs are often not verifiable in-cluster).

## Cloudflare Tunnel config (token-based, remote-managed)

- The tunnel uses a token (`TUNNEL_TOKEN` secret), which means **Cloudflare's remote config overrides the local ConfigMap**.
- Editing [`charts/cloudflared/values.yaml`](charts/cloudflared/values.yaml:23) updates the ConfigMap but the tunnel daemon ignores it in favour of the remote config.
- To change ingress routes, you must **also** PUT via the Cloudflare API:
  ```bash
  source .env && curl -s -X PUT \
    "https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/cfd_tunnel/${CF_TUNNEL_ID}/configurations" \
    -H "X-Auth-Email: ${CLOUDFLARE_EMAIL}" -H "X-Auth-Key: ${CLOUDFLARE_GLOBAL_API_KEY}" \
    -H "Content-Type: application/json" \
    --data '{"config":{"ingress":[...], "warp-routing":{"enabled":true}}}'
  ```
- Keep the local ConfigMap in sync as documentation / fallback, but the API call is what actually takes effect.

## Acestream-scraper API quirks

- The scraper uses the upstream `pipepito/acestream-scraper:latest` Docker image. The Flask `app/` package is **not in git** — it's baked into the image (built from `pyproject.toml` during Docker build).
- The `app/` directory is generated by `pip install -e .` locally; source routes live inside the installed package.
- **Config API endpoint**: `PUT /api/config/<key>` (not `/api/v1/config/`). Each endpoint expects a specific JSON key:
  ```bash
  # Set base URL (used in playlist.m3u output)
  curl -X PUT https://scraper.tonioriol.com/api/config/base_url \
    -H "Content-Type: application/json" \
    -d '{"base_url":"https://ace.tonioriol.com/ace/getstream?id="}'

  # Set Ace Engine URL
  curl -X PUT https://scraper.tonioriol.com/api/config/ace_engine_url \
    -H "Content-Type: application/json" \
    -d '{"ace_engine_url":"http://localhost:6878"}'

  # Set rescrape interval (key is "hours", not "rescrape_interval")
  curl -X PUT https://scraper.tonioriol.com/api/config/rescrape_interval \
    -H "Content-Type: application/json" \
    -d '{"hours":6}'
  ```
- The **Config web UI** (`/config`) is a React SPA. If the "Update" button appears to do nothing (value disappears from the field), use the API directly — the frontend may have a bug with certain URL formats.
- **URL management**: `POST /api/urls/` to add, `DELETE /api/urls/{id}` (no trailing slash) to remove. Trailing slash on DELETE returns 404.
- **Channel data**: `GET /api/channels/` returns all channels; filter with `ch.status === 'active' && ch.is_online !== false`.
- **Ports**: Flask on `8000`, Acexy on `8080`, Acestream Engine on `6878` (all in the same pod).
- **Acexy vs raw engine**: Acexy (`ace.tonioriol.com` → port 8080) is a Go proxy that wraps the engine API. Key differences:
  - Acexy **rejects the `pid` parameter** with HTTP 400 ("PID parameter is not allowed"). Never include `&pid=` in Acexy URLs.
  - Acexy only supports MPEG-TS via `/ace/getstream?id=<hash>` — no HLS (`/ace/manifest.m3u8` returns 404).
  - The raw engine (port 6878) supports both HLS and MPEG-TS, and accepts `pid`.
  - The scraper Config page has an "Add PID parameter to URLs" checkbox — must be **unchecked** when using Acexy.
- **Config is stored in SQLite** (`/app/config/acestream_scraper.db`), persisted via PVC at `/app/config`.

## Acestream-scraper release process (CI pipeline)

- The scraper repo (`tonioriol/acestream-scraper`) is a fork of `pipepito/acestream-scraper` with custom overlay code in `app/`.
- CI/CD: [`.github/workflows/release.yml`](../acestream-scraper/.github/workflows/release.yml:1) runs **semantic-release** on every push to `main` (when `app/`, `migrations/`, `Dockerfile`, `requirements*.txt`, `wsgi.py`, etc. change).
- **NEVER** deploy manually (no `docker build`, no `docker push`, no `kubectl rollout restart`). Just push to `main` with Conventional Commits.

### Deploy steps

1. Make code changes in the `acestream-scraper` repo. Use **Conventional Commits** (`fix:`, `feat:`, `BREAKING CHANGE:`) so semantic-release can compute the version:
   ```bash
   cd ../acestream-scraper
   git add -A && git commit -m "fix: description of change"
   git push origin main
   ```

2. That's it. The CI pipeline handles everything:
   - **semantic-release** bumps the version, creates a GitHub release + git tag
   - **Docker build** builds `linux/amd64` image and pushes to `ghcr.io/tonioriol/acestream-scraper:vX.Y.Z`
   - **ArgoCD Image Updater** detects the new semver tag (polls every ~2 min) and rolls the deployment

3. Verify deployment:
   ```bash
   export KUBECONFIG=${PWD}/clusters/neumann/kubeconfig
   # Check Image Updater logs
   kubectl logs -l app.kubernetes.io/name=argocd-image-updater --tail=20 -n argocd
   # Check current image
   kubectl get deploy acestream-scraper -o jsonpath='{.spec.template.spec.containers[*].image}'
   ```

### Important notes

- The Image Updater is configured via the `ImageUpdater` CRD in [`apps/argocd-image-updater.yaml`](apps/argocd-image-updater.yaml:58) (not via Application annotations).
- The `semver` strategy + `allowTags: regexp:^v?\d+\.\d+\.\d+$` means only proper semver tags are considered.
- The chart [`charts/acestream-scraper/values.yaml`](charts/acestream-scraper/values.yaml:1) has a baseline tag but Image Updater overrides this at deploy time.
- GHCR auth: the CI pipeline uses `GHCR_PAT` secret; the cluster uses `ghcr-pull` Secret in the `argocd` namespace.
- If SSH to github.com times out, use HTTPS: `git push https://github.com/tonioriol/acestream-scraper.git main`

## kubectl context (important for agents)

- Always use the neumann cluster kubeconfig when running `kubectl` or `helm` commands: `KUBECONFIG=${PWD}/clusters/neumann/kubeconfig`.
- In terminal commands, prefix with `export KUBECONFIG=${PWD}/clusters/neumann/kubeconfig &&` or pass `--kubeconfig clusters/neumann/kubeconfig` to avoid hitting the wrong cluster context.
